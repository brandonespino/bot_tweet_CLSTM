{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import numpy as np\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and labels \n",
    "with open('tweet_embeddings25k.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "with open('labels25k.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "#converting to tensor/adding labels\n",
    "dataset = torch.stack(dataset)\n",
    "labels = torch.tensor(labels.tolist())\n",
    "dataset = torch.utils.data.TensorDataset(labels, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10043\n",
      "9957\n",
      "2456\n",
      "2544\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_labels = train_dataset[:][0]\n",
    "train_data = train_dataset[:][1]\n",
    "\n",
    "test_labels = test_dataset[:][0]\n",
    "test_data = test_dataset[:][1]\n",
    "\n",
    "# check balance of dataset\n",
    "# TODO: check in 'Training Model' if I actually need to \n",
    "# convert to labels to list or leave as tensor\n",
    "train_labels = train_labels.tolist()\n",
    "test_labels = test_labels.tolist()\n",
    "print(train_labels.count(0))\n",
    "print(train_labels.count(1))\n",
    "print(test_labels.count(0))\n",
    "print(test_labels.count(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLSTM(\n",
      "  (conv1): Conv1d(1, 20, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "  (conv2): Conv1d(20, 10, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "  (lstm): LSTM(48, 48, num_layers=2, bidirectional=True)\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=40, bias=True)\n",
      "  (fc3): Linear(in_features=40, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLSTM,self).__init__()\n",
    "        # expects input tensor of [1,1,768]\n",
    "        # 1 input channel, 20 output channels, 1x5 convolution with padding\n",
    "        self.conv1 = nn.Conv1d(1,20,5,stride = 2,padding = 2)\n",
    "        # 20 input channel, 10 output channels, 1x5 convolution\n",
    "        self.conv2 = nn.Conv1d(20,10,5,stride = 2,padding = 2)\n",
    "        # end up with tensors [1,10,192]\n",
    "        # these turn into tensors [1,10,48] after pooling which \n",
    "        # are turned into a list of 10 [1,48] tensors to pass into the lstm\n",
    "        # each of these 10 will be taken in as token representations\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = 48,hidden_size = 48,num_layers = 2,bidirectional = True)\n",
    "        # the lstm will output tuple of ([4,1,48],[4,1,48]) where the first entry is\n",
    "        # the hidden state for classification and the second entry is the cell state\n",
    "\n",
    "        self.fc1 = nn.Linear(4*48,120)\n",
    "        self.fc2 = nn.Linear(120,40)\n",
    "        self.fc3 = nn.Linear(40,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #set initial hidden state and cell state for lstm\n",
    "        # 2 is for the number of layers and the other 2 is because of bidirection\n",
    "        hidden = torch.randn(2*2,1,48)\n",
    "        cell = torch.randn(2*2,1,48)\n",
    "        \n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)),kernel_size = 2,stride = 2)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)),kernel_size = 2,stride = 2)\n",
    "        # x is now a tensor of shape [1,10,48] because we did the pooling\n",
    "        \n",
    "        # now put the \"tokens\" into a list\n",
    "        tweet_representation = []\n",
    "        for i in range(len(x[0])):\n",
    "            tweet_representation.append(x[0][i].unsqueeze(0))\n",
    "        # tweet_representation is a list of the ten [1,48] tensors to\n",
    "        # be passed into the lstm\n",
    "        for j in tweet_representation:\n",
    "            _, hidden_tuple = self.lstm(j.view(1,1,-1),(hidden,cell)) \n",
    "\n",
    "        # hidden state tensor [4,1,48]\n",
    "        hidden_state = hidden_tuple[0]\n",
    "        # flatten the hidden_state to tensor [192]\n",
    "        x = hidden_state.reshape(-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x is now 1 x 120\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x is now 1 x 30\n",
    "        x = self.fc3(x)\n",
    "        # x is now a single predicted class\n",
    "        return torch.sigmoid(x)\n",
    "model = CLSTM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5358], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# small test of the model---------------\n",
    "inp = torch.randn(1,1,768)\n",
    "out = model(inp)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr = .0005)\n",
    "\n",
    "# #10 is the number of epochs\n",
    "# total_step = len(train_data)\n",
    "# for epoch in range(5):\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(train_data,0):\n",
    "#         #inputs.shape is [1,1,768]\n",
    "#         inputs = data\n",
    "#         #label is int (1 or 0)\n",
    "#         label = train_labels[i]\n",
    "#         label = torch.tensor([float(label)])\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs = inputs.unsqueeze(0).unsqueeze(0)\n",
    "#         output = model(inputs)\n",
    "#         loss = criterion(output,label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         #print stats\n",
    "#         running_loss +=loss.item()\n",
    "#         if (i+1) % 100 == 0:\n",
    "#             print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Output [{}], True label [{}]' \n",
    "#                    .format(epoch+1, 100, i+1, total_step, loss.item(),output.item(),label.item()))\n",
    "#             running_loss = 0.0\n",
    "# print('finished training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on 5000 test tweets: 86 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_data,0):\n",
    "        #inputs.shape is [1,1,768]\n",
    "        inputs = data\n",
    "        #label is int (1 or 0)\n",
    "        label = test_labels[i]\n",
    "        label = torch.tensor([float(label)])\n",
    "        inputs = inputs.unsqueeze(0).unsqueeze(0)\n",
    "        output = model(inputs)\n",
    "        if(output >=.5):\n",
    "            output = 1\n",
    "        else:\n",
    "            output = 0\n",
    "        total+=1\n",
    "        correct+= int(output == label)\n",
    "        \n",
    "print('Accuracy of the model on 5000 test tweets: %d %%' % (100*correct/total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
