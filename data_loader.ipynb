{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import emoji\n",
    "import numpy as np\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing \n",
    "Removal of:\n",
    "- emojis\n",
    "- #'s\n",
    "- @'s\n",
    "- links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1  @GodNegus: #IHatePokemonGoBecause i have not s...\n",
      "1      1  @LibertyNow10: .@SpeakerRyan when you come up ...\n",
      "2      1  John Podesta Trends on Facebook After Peter Sc...\n",
      "---------------------------\n",
      "   label                                               text\n",
      "0      1            i have not seen one wild charmander yet\n",
      "1      1   . when you come up with better solution then ...\n",
      "2      1  John Podesta Trends on Facebook After Peter Sc...\n"
     ]
    }
   ],
   "source": [
    "# dataset = pd.read_csv('twitter_data/tweets25k.csv')\n",
    "\n",
    "# # first 3 elements\n",
    "# print(dataset.head(3))\n",
    "\n",
    "# # removing #'s, @'s, http's and emojis\n",
    "# def emoji_free_text(text):\n",
    "#     text.encode('ascii','ignore').decode('ascii')\n",
    "#     text = re.sub(emoji.get_emoji_regexp(), r'', text)\n",
    "#     return text\n",
    "         \n",
    "# for i in range(0,len(dataset)):\n",
    "#     dataset.iloc[i,1] = re.sub(r\"@\\S+\",\"\",str(dataset.iloc[i,1]))\n",
    "#     dataset.iloc[i,1] = re.sub(r\"http\\S+\",\"\", dataset.iloc[i,1])\n",
    "#     dataset.iloc[i,1] = re.sub(r\"#\\S+\",\"\", dataset.iloc[i,1])\n",
    "#     dataset.iloc[i,1] = emoji_free_text(dataset.iloc[i,1])\n",
    "# print(\"---------------------------\")\n",
    "# # first 3 elements after removal\n",
    "# print(dataset.head(3))\n",
    "# dataset.to_pickle('twitter_processed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each encoded_layers in encoded_layers_list has \n",
    "\n",
    "1) The layer number (12 layers)\n",
    "\n",
    "2) The batch number (1 sentence)\n",
    "\n",
    "3) The word / token number (n tokens in our sentence)\n",
    "\n",
    "4) The hidden unit / feature number (768 features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load dataset\n",
    "dataset = pd.read_pickle(\"twitter_processed.pkl\") \n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_list = []\n",
    "\n",
    "for tweet in dataset.text.to_list():\n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \"+ tweet + \" [SEP]\"\n",
    "    # Split the sentence into tokens.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    # Map the token strings to their BERT vocabulary indices.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # for single-sentence input mark each token as belonging to sentence \"1\".\n",
    "    segments_ids = [1]*len(tokenized_text)\n",
    "    # converting to pytorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor,segments_tensors)\n",
    "    # Each layer in encoded_layers list is a torch tensor where \n",
    "    # shape for each of the 12 layers:  torch.Size([1, num_tokens, 768])\n",
    "    # we will be averaging the second to last hidden layer of each token to get a single\n",
    "    # [1,768] size vector as the sentence embedding (experiment with this)\n",
    "    token_vecs = encoded_layers[11][0]\n",
    "    # Calculate the average of all token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    sentence_embeddings_list.append(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle data for use in model\n",
    "with open('tweet_embeddings25k.pkl', 'wb') as f:\n",
    "    pickle.dump(sentence_embeddings_list, f)\n",
    "#pickle labels for later use\n",
    "with open('labels25k.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset['label'], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
